---
title: "Final 2024 SCGL Distance Script"
author: "Jonathan Amegin"
date: "2024-09-24"
output: html_document
---

# [Sclerocactus Distance Sampling]{.underline}

End goals:

-   Plot the individual observations for each site on a coordinate grid
-   Mark the observations which overlap/were observed both going NS & EW.
-   Get a percent observational error to apply across the observed population.
-   Distribution of observations based on distance from line

Site names: 3 & 1= Picnic, 10= Bridgport, 12= Fram

## Setting Up

```{r message=FALSE, warning=FALSE, include=FALSE}
# tidyverse contains both 'dyplr' and 'ggplot2'
library(tidyverse)
library(dplyr)
library(janitor)
library(ggrepel)

setwd("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/hackathon/ScleroDistanceSampling")
scgl_obs <- read_csv("Data/distanceSampling_1.csv")
scgl_trans_length <- read_csv("Data/transect_2.csv")

# scgl_obs <- read_csv("data_raw/distanceSampling_1.csv")
# scgl_trans_length <- read_csv("data_raw/transect_2.csv")

scgl_obs <- clean_names(scgl_obs)
```


This function will be used later to find duplicate plants

```{r test function}

# `site` = a dataframe with relative (x,y) coords as seperate columns `x` & `y`, transect directions as `NS` & `EW`, and unique `plant_id' for each observation.
# `b` = an allowance in meters(m) within which two observations (1 NS & 1 EW) will be considered the same observation
crossed <- function(site, b){
  
 temp_site <-  site %>% 
    rowwise() %>% 
    mutate(coord = list(c(x,y)))
  
#chart all possible combinations of NS coordinates and EW coordinates
 grid <-  expand.grid(EW = temp_site[which( site$transect_orientation == "EW"), "coord"]$coord,
                      NS = temp_site[which( site$transect_orientation == "NS"), "coord"]$coord) %>% 
    rowwise() %>% # to look for the differences between EW and NS points by row
   #make a column of distances between points
    mutate( dist = sqrt(sum((EW - NS)^2))) %>% 
   #filter the combinations to only the ones within the supplied fuzziness `b`
    filter(dist <= b)

#appending to contain columns with EW plant_id's for each combination
 grid <- left_join(grid, temp_site[, c("coord", "plant_id")],
                   by = join_by(EW == coord)) %>% 
   mutate(EW_ID = plant_id, plant_id = NULL) %>% 
  #appending to contain columns with NS plant_id's for each combination
   left_join(temp_site[, c("coord", "plant_id")], 
             by = join_by(NS == coord)) %>% 
   mutate(NS_ID = plant_id, plant_id = NULL)

#checking for Id's with multiple matches.(Should be only one distinct NS observation for each distinct EW observation). Keep only the match with the least distance between points
if(!(all(count(grid, NS_ID)$n >1) | all(count(grid, EW_ID)$n >1)) ){
  grid <- filter(group_by(grid, NS_ID), dist == min(dist))
  grid <- filter(group_by(grid, EW_ID), dist == min(dist))
  }
 
 #Within the original dataframe, change corresponding EW ID's to their NS counterparts
 temp_site <- left_join(temp_site, grid[, c("EW_ID", "NS_ID")], by = join_by(plant_id == EW_ID)) %>% 
   mutate( plant_id = ifelse( !is.na(NS_ID), NS_ID, plant_id), NS_ID = NULL, coord = NULL) %>% 
   ungroup()
 
 temp_site
 
}



```


Start here
```{r}
scgl <- scgl_obs[!(is.na(scgl_obs$transect_meter_mark|scgl_obs$distance_to_group)),] %>% #filtering NA's and outliers. 
        filter(transect_meter_mark < 110, distance_to_group < 10) %>% 
        mutate( object_id = as.factor(object_id),
                site_name = as.factor(site_name),
                transect_number = as.factor(transect_number))

            
scgl1 <- scgl %>% 
   mutate(
     # EW transect meter marks need to be * -1 because they were recorded as if in the(+,+) quadrant, but the mapping we will do is based on (-,+) quadrant. 
    transect_meter_mark = case_when( 
      transect_orientation == "EW" ~ transect_meter_mark * -1, 
      TRUE ~ transect_meter_mark),
    # making the 'direction to group' indicate if the 'distance to group' needs to be left/right of transect.  
    distance_to_group = case_when(
      direction_to_group == "-" ~ distance_to_group * -1, 
      TRUE ~ distance_to_group ) 
          )
           

# setting up the (X,Y) columns to then transform based on plot/shapefile analysis
scgl1 <- scgl1 %>% 
  mutate( x = case_when(transect_orientation == "NS" ~ distance_to_group, transect_orientation == "EW" ~ transect_meter_mark), 
          y = case_when(transect_orientation == "EW" ~ distance_to_group, transect_orientation == "NS" ~ transect_meter_mark)) 

scgl1 <- left_join(scgl1, scgl_trans_length[, c("Transect Length", "GlobalID")], by = join_by(parent_global_id == GlobalID))                
                
#filtering to sites
fram <- scgl1 %>% 
  filter(site_name == "12")

picnic <- scgl1 %>% 
  filter(site_name == "1"| site_name == "3")

bridgeport <- scgl1 %>% 
  filter(site_name == "10")

oilpad <- scgl1 %>% 
  filter(site_name == "11")

```

The raw data needs to be made into a coordinate-grid-friendly format. Each site needs to have the coordinates transformed based on the layout of the transects on the plot.These layouts were recorded via shapefile, so the shapefiles were measured seperately and compared to recorded information to figure out the necessary transformations to the data.

## [Site: Fram]{.underline}

### Plotting individuals

Involves transforming the coordinates to match the plot design.

```{r fram transform}
#transforming Fram coordinates to accurately reflect plot design.This is based on the origin (0,0) being in the lower right of the plot so that one or both of the axis matches the start of at least one transect. In this case, the origins of the three NS transects are at Y = 0, and 4 out of 5 EW transects' origins are at X = 0.
fram_1 <- fram %>% 
  mutate( 
    x = case_when(transect_orientation == "NS" & transect_number == "1" ~ x - 10, 
                  transect_orientation == "NS" & transect_number == "2" ~ x - 20,
                  transect_orientation == "NS" & transect_number == "3" ~ x - 30,
                  transect_orientation == "EW" & transect_number == "5" ~ x - 10,
                  TRUE ~ x
                  ),
    y = case_when(transect_orientation == "EW" & transect_number == "1" ~ y + 10,
                  transect_orientation == "EW" & transect_number == "5" ~ y + 50,
                  transect_orientation == "EW" & transect_number == "7" ~ y + 70,
                  transect_orientation == "EW" & transect_number == "8" ~ y + 80,
                  transect_orientation == "EW" & transect_number == "9" ~ y + 90,
                  TRUE ~ y),
    )

#Fram NS lines tapes were laid backwards making 0 = 100. 
fram_1 <- fram_1 %>% 
  mutate(
    y = case_when(transect_orientation =="NS" ~ ((y - 100)* -1),
                  TRUE ~ y) )

#transect start/stops for mapping
fram_lines <- tibble( x = c(-10, -20, -30, 0, -10, 0, 0, 0), 
                      y = c(0, 0, 0, 10, 50, 70, 80, 90), 
                      xend = c(-10, -20, -30, -33, -33, -60, -65, -39), 
                      yend = c(100, 100, 100, 10, 50, 70, 80, 90),
                      label = c("1NS", "2NS", "3NS", "1EW", "5EW", "7EW", "8EW", "9EW")
                    )


 fram_plot<-  ggplot( data = fram_1, mapping = aes(x = x, y = y)) +
  geom_point() +
  geom_segment(data = fram_lines, aes(x = x, y = y, xend = xend, yend = yend)) +
   geom_text(data = fram_lines,
            aes( label = label),
            nudge_x = 4,
            size = 2,
            )

  
  fram_plot +
    geom_point(data = fram_1, aes( colour = transect_orientation))+
    coord_fixed(ratio = 1, xlim = c(-75, 10), ylim = c(0, 100))
```

### Marking Overlap

For the initial marking of overlapping observations we will use an **allowance of 0.5m.**

`plant_id` = unique identifiers for each observation

`zoned` = is observation within the 3m/*3m square at transect intersections? (1 or 0)

```{r fram dupes and plot}
#add unique plant ID's for each observation =`plant_id'
fram_id <- mutate(fram_1, plant_id = c(1:nrow(fram_1)))

# add a variable to indicate if an observation is within the 3m*3m square at the intersections of NS & EW transects. 
# `zoned` column: 1 = within zone, 0 = not within zone
fram_id <- mutate(fram_id, zoned = case_when( c(rowSums(mapply( FUN = function(a, b){between(a, b-3, b+3)}, fram_id[, "y"], c(10, 50, 70, 80, 90))) +
                                                rowSums(mapply( FUN = function(a, b){between(a, b-3, b+3)}, fram_id[, "x"], c(-10, -20, -30)))) == 2 ~ 1,
       TRUE ~ 0)) 

#using the custom function to find overlapping observations   
fram_duped <- crossed(filter(fram_id), 1)  # a = plot data w/ID's, b = fuzziness

fram_plot1 <- fram_duped %>% 
  filter(zoned == 1) %>% #show only zoned observations
  filter( duplicated(plant_id)) %>% #show only overlapping observations
  ggplot( mapping = aes( x = x, y = y)) +
  geom_point(color = "blue",) +
  geom_point(data = filter(fram_duped, zoned == 1), shape = 1, aes( color = transect_orientation) ) + #plot all zoned observations
  geom_segment(data = fram_lines, aes(x = x, y = y, xend = xend, yend = yend)) +
  coord_fixed(ratio = 1, xlim = c(-75, 10), ylim = c(0, 100)) +
  geom_text_repel( data = filter(fram_duped, duplicated(plant_id)), aes( label = plant_id), size = 3, min.segment.length = 0.1) +  # label the overlapping observations
  geom_text(data = fram_lines,
            aes( label = label),
            nudge_x = 4,
            size = 2,
            )


fram_plot1  

  
```


```{r}

site <- fram_id
b <- 1

 temp_site <-  site %>%
 rowwise() %>%
 mutate(coord = list(c(x,y)))

 
temp_site %>%
  select(site_name, transect_orientation, x, y, coord)


 grid <-  expand.grid(EW = temp_site[which( site$transect_orientation == "EW"), "coord"]$coord,
                      NS = temp_site[which( site$transect_orientation == "NS"), "coord"]$coord) %>% 
    rowwise() %>% # to look for the differences between EW and NS points by row
   #make a column of distances between points
    mutate( dist = sqrt(sum((EW - NS)^2))) %>% 
   #filter the combinations to only the ones within the supplied fuzziness `b`
    filter(dist <= b)

grid$dist 

grid <- left_join(grid, temp_site[, c("coord", "plant_id")],
                  by = join_by(EW == coord)) %>% 
  mutate(EW_ID = plant_id, plant_id = NULL) %>% 
  #appending to contain columns with NS plant_id's for each combination
  left_join(temp_site[, c("coord", "plant_id")], 
            by = join_by(NS == coord)) %>% 
  mutate(NS_ID = plant_id, plant_id = NULL)

grid %>% select(EW_ID, NS_ID) %>% ## These are just the row numbers for each
  arrange(EW_ID)

if(!(all(count(grid, NS_ID)$n >1) | all(count(grid, EW_ID)$n >1)) ){
  grid <- filter(group_by(grid, NS_ID), dist == min(dist))
  grid <- filter(group_by(grid, EW_ID), dist == min(dist))
  }

```



Dark blue points are overlapping observations based on fuzziness provided in the `crossed()` function

### Observational Error

Use a loop to filter down to only cactus that *should* have been seen both going EW & NS. The Observational Error calculated will be applied separately to NS & EW actual observations and then averaged together. This is meant to avoid double counting of overlapping observations.

```{r fram cross sectioned}
#  error comes from the number if plants were seen going NS AND EW divided by number if plants seen going NS OR EW within the 3 meter crossing zone

ob_error_fram <- (nrow(filter(fram_duped, duplicated(plant_id)))*2 / nrow(filter(fram_duped, zoned == 1)))
```



# Lincoln-Petersen estimator  
Assume detections by each observer are independent     
      - Banneheka et al 1997 - Stratified Two-Sample Tag-Recovery Census of Closed Populations     
      - Mares et al 1981: N = n_i M_i / m_i where samples are i, n is number captured in sample i, m is number of previously marked animals captured in sample i, M is the number of marked in population during sample i
      - Burt et al 2014: probability of recapture given detection p_1|2 = m/n_2 where m is the number of duplicates, n_2 are the number that observer 2 detected


```{r}

fram_duped %>%
  select(distance_to_group, number_seedlings_in_group, number_vegetative_in_group, number_reproductive_in_group, 
         transect_orientation ,x, y, plant_id, zoned) %>%
ggplot(  aes(x,y, color = transect_orientation, shape = transect_orientation)) +
  geom_point()+
  scale_shape_manual(values = c(3,4))+
  scale_color_manual(values = c("black","skyblue"))+
  theme_bw()

# Burt et al 2014
n_ew <- fram_duped %>%
  filter(zoned == 1) %>%
  filter(transect_orientation == "EW") %>%
  summarise(n_ew = n_distinct(plant_id))

## previously marked that were seen in both
m_ns <- fram_duped %>%
  filter(zoned == 1) %>%
  group_by(plant_id) %>%
  summarise(m = n()) %>%
  filter(m > 1) %>%
  summarise(n = n())

M <- length(unique(fram_duped$plant_id[fram_duped$zoned == 1]))

N <- (n_ew*M)/m_ns

## But what I want is the detection probability

## For logistic regression, success of NS seeing ones in EW and then of EW seeing ones from NS  
obs_trials <- fram_duped %>%
  select(distance_to_group, number_seedlings_in_group, number_vegetative_in_group, number_reproductive_in_group, 
         transect_orientation ,x, y, plant_id, zoned) %>%
  filter(zoned == 1) %>%
  mutate(m = length(duplicated(plant_id)),
         EW_2 = )


```


### Changing fuzziness

```{r}

#make a curve that shows how the number of corrected observations changes as you change the fuzziness

fuzziness <- c(seq(0, 3, by = 0.1))
 
      error = fuzziness
      n_duplicated = fuzziness
      n_actual = fuzziness 
      n_corrected = fuzziness
    
    
for (fuzz in fuzziness){

fram_crazy <- crossed(fram_id, fuzz)

temp_error <-  which(error == as.character(fuzz))  
   error[which(error == as.character(fuzz))] <- (nrow(filter(fram_crazy, duplicated(plant_id)))*2 / nrow(filter(fram_id, zoned == 1)))

   n_duplicated[which(n_duplicated == as.character(fuzz))] <- sum(duplicated(fram_crazy$plant_id))
    
   n_actual[which(n_actual == as.character(fuzz))] <- n_distinct(fram_crazy$plant_id)
   
   n_corrected[temp_error] <- mean( c(n_distinct(which(fram_crazy$transect_orientation == "NS"))/error[temp_error],
                                       n_distinct(which(fram_crazy$transect_orientation == "EW"))/error[temp_error]))

}
      
fram_fuzz <- tibble(fuzziness, error, n_actual, n_duplicated, n_corrected)
 
```

```{r}
  
pivot_longer(fram_fuzz, cols = !fuzziness, names_to= "type", values_to= "count") %>% 
  group_by(type) %>% 
  ggplot( aes(fuzziness, count, color = type))+
  geom_line() +
  labs( x = "Fuzziness (m)")

  


```

This chart shows the change in the calculated number of *overlapping*, *unique*, and *corrected* total observations as the fuzziness is increased from 0m to 3m.

## [Site: Picnic]{.underline}

### Plotting individuals

Involves transforming the coordinates to match the plot design.

```{r picnic transform and plot}

#describing the plot design
pic_lines <- tibble( x = c(-20, -30, -40, -20, -10, -10, 0), 
                     y = c(20, 10, 0, 20, 30, 40, 50), 
                     xend = c(-20, -30, -40, -90, -70, -70, -50),
                     yend = c(60, 76, 50, 20, 30, 40, 50),
                     label = c("1NS", "2NS", "3NS", "1EW", "2EW", "3EW", "4EW"))
                
 picnic_1 <-  picnic %>% 
   mutate( 
    x = case_when(transect_orientation == "NS" & transect_number == "1" ~ x - 20, 
                  transect_orientation == "NS" & transect_number == "2" ~ x - 30,
                  transect_orientation == "NS" & transect_number == "3" ~ x - 40,
                  transect_orientation == "EW" & transect_number == "1" ~ x - 20,
                  transect_orientation == "EW" & transect_number == "2" ~ x - 10,
                  transect_orientation == "EW" & transect_number == "3" ~ x - 10,
                  transect_orientation == "EW" & transect_number == "4" ~ x
                  ),
    y = case_when(transect_orientation == "NS" & transect_number == "1" ~ y + 20, 
                  transect_orientation == "NS" & transect_number == "2" ~ y + 10,
                  transect_orientation == "NS" & transect_number == "3" ~ y ,
                  transect_orientation == "EW" & transect_number == "1" ~ y + 20,
                  transect_orientation == "EW" & transect_number == "2" ~ y + 30,
                  transect_orientation == "EW" & transect_number == "3" ~ y + 40,
                  transect_orientation == "EW" & transect_number == "4" ~ y + 50,
                  )
   )
 
pic_plot <-  picnic_1 %>% 
  ggplot( mapping = aes(x = x, y = y)) +
  geom_point(aes(color = transect_orientation)) +
  geom_segment(data = pic_lines, aes(x = x, y = y, xend = xend, yend = yend))
pic_plot

```

### Marking Overlap

```{r}

#apply unique ID's
picnic_id <- mutate(picnic_1, plant_id = c(1:nrow(picnic_1)))

#filter to crozzing zones
picnic_id <- mutate(picnic_id, zoned = case_when( c(rowSums(mapply( FUN = function(a, b){between(a, b-3, b+3)}, picnic_id[, "y"], c(20, 30, 40, 50))) +
                                                    rowSums(mapply( FUN = function(a, b){between(a, b-3, b+3)}, picnic_id[, "x"], c(-20, -30, -40)))) == 2 ~ 1,
       TRUE ~ 0)) 

#find duplicates within those zones
picnic_duped <- crossed(filter(picnic_id), 0.5)

pic_plot1 <- picnic_duped %>% 
  filter(zoned ==1) %>% 
  ggplot( mapping = aes(x, y)) +
  geom_point(aes(color = transect_orientation), shape = 1) +
  geom_point(data = filter(picnic_duped, duplicated(plant_id)), color = "blue") +
  geom_segment(data = pic_lines, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_text_repel(data = pic_lines,
            aes( label = label),
            size = 2,
            ) +
   geom_text_repel( data = filter(picnic_duped, duplicated(plant_id)), aes( label = plant_id), size = 3, min.segment.length = 0.1)  # label the overlapping observations
pic_plot1

```

https://distancesampling.org/mrds/articles/mrds-golftees.html 
Follow example and make data like mrds 
```{r}
library(mrds)

data("book.tee.data")

book.tee.data$book.tee.dataframe  # List 1

picnic.data <- picnic_duped %>%
  filter(zoned == 1) %>%
  mutate(detected = 1) %>%
  mutate(distance = abs(distance_to_group)) %>%
  rowwise() %>%
  mutate(size = sum(c(number_seedlings_in_group, number_vegetative_in_group, number_reproductive_in_group), na.rm = TRUE)) %>%
  ungroup() %>%
  dplyr::rename("observer" = "transect_orientation") %>%
  dplyr::select(plant_id, observer, detected, ## detected by fill all plant_id for each trans_ori
         distance, size, transect_meter_mark) %>%
  complete(nesting(plant_id,size),observer, fill = list(detected = 0)) %>% ## Keep only size and plantID that are in the data
  relocate(size, .after = distance) %>%
  group_by(plant_id) %>%
  mutate(distance = case_when(any(is.na(distance)) & abs(transect_meter_mark) < 15 ~ abs(10-max(transect_meter_mark, na.rm = TRUE)),
      any(is.na(distance)) & abs(transect_meter_mark) > 15 & abs(transect_meter_mark) < 25 ~ abs(20-max(transect_meter_mark, na.rm = TRUE)),
      any(is.na(distance)) & abs(transect_meter_mark) > 25 & abs(transect_meter_mark) < 35 ~ abs(30-max(transect_meter_mark, na.rm = TRUE)),
      any(is.na(distance)) & abs(transect_meter_mark) > 35 & abs(transect_meter_mark) < 45 ~ abs(40-max(transect_meter_mark, na.rm = TRUE)),
      any(is.na(distance)) & abs(transect_meter_mark) > 45 & abs(transect_meter_mark) < 55 ~ abs(50-max(transect_meter_mark, na.rm = TRUE)),
      any(is.na(distance)) & abs(transect_meter_mark) > 55 & abs(transect_meter_mark) < 65 ~ abs(60-max(transect_meter_mark, na.rm = TRUE)),
      TRUE ~ max(distance, na.rm = TRUE)
                              )) %>%
  select(-transect_meter_mark) %>%
  mutate(zoned = 1)

  
picnic.region <- data.frame(Region.Label = "Picnic", 
                            Area = 7802)

picnic.samples <- picnic %>%
  distinct(transect_number, transect_orientation, `Transect Length`) %>%
  rowwise() %>%
  mutate(Sample.Label = paste0(transect_number,transect_orientation)) %>%
  ungroup() %>%
  mutate(Region.Label = "Picnic") %>%
  rename("Effort" = "Transect Length") %>%
  select(Sample.Label, Region.Label, Effort) %>%
  mutate(Effort = case_when(Sample.Label == "1EW" ~ 70,
                            TRUE ~ Effort)) %>%
  filter(!is.na(Effort)) %>%
  arrange(Sample.Label)

picnic.obs <- picnic_duped %>%
  filter(zoned == 1) %>%
  rowwise() %>%
  mutate(Sample.Label = paste0(transect_number,transect_orientation)) %>%
  ungroup() %>%
  distinct(plant_id, Sample.Label) %>%
  mutate(Region.Label = "Picnic")

```


the distance will be different and I should be able to estimate it by subtracting the nearest meter marker
```{r}

detections <- picnic.data %>%
  rename(object = plant_id)

detections_all <- picnic.data <- picnic_duped %>%
  filter(zoned == 0) %>%
  mutate(detected = 1) %>%
  mutate(distance = abs(distance_to_group)) %>%
  rowwise() %>%
  mutate(size = sum(c(number_seedlings_in_group, number_vegetative_in_group, number_reproductive_in_group), na.rm = TRUE)) %>%
  ungroup() %>%
  dplyr::rename("observer" = "transect_orientation") %>%
  rename(object = plant_id) %>%
  dplyr::select(object, observer, detected, distance, size, zoned) %>%
  bind_rows(detections) %>%
  ## Three with zero size??
  mutate(size = case_when(size ==0 ~ 1,
                          TRUE ~ size))
  
##  <https://distancedevelopment.r-universe.dev/mrds/doc/manual.html#mrds-package> 
## Independent observer configuration 
in.mr.dist.EW <- ddf(
                  dsmodel = ~ mcds(key = "hn", formula = ~ distance + size),
                  # mrmodel = ~ glm(link = 'logit', formula = ~ distance + size),
                  data = detections_all[detections_all$observer == "EW",], 
                  method = "ds",
                  meta.data = list(width = 3))

plot(in.mr.dist.EW)

in.mr.dist.NS <- ddf(
                  dsmodel = ~ mcds(key = "hn", formula = ~ distance + size),
                  # mrmodel = ~ glm(link = 'logit', formula = ~ distance + size),
                  data = detections[detections$observer == "NS",], 
                  method = "ds",
                  meta.data = list(point = FALSE, binned = FALSE, width = 3))

plot(in.mr.dist.NS)

```


Simple DS   
Kery and Royle 2016 Ch 8     

      1. Individual site membership as a categorical covariate      
      2. specify site membership covariate as a categorical random variable with cell probabilities proportional to the Poisson mean parameter lambda_s    
      3. Covariate effects directly on lambda - as in usual binomial and multinomial N-mixture models     
      4. Covarites (size) on the detection model by specifying a linear model for log(sigma)  [they used wind speed as a site covariate; we have size for each individual as an observational covariate]          
           i. y[i] ~ dbern(p[i])   relationship for individuals     
           ii. p[i] ~ exp(-d[i]*d[i] / (2*sigma[site[i]]*sigma[site[i]]))   relationship between individuals and an individual-specific covariate distance and a site-specific paramter sigma[site[i]]       
           iii. log(sigma[site[i]]) <- alpha0 + alpha1 * wind[site[i]]      
 
Chapter 9 ppg. 463-487       

      1. Aggregation - if one in group is detected, others in the group or the whole group will be detected.    
           i. Violates key assumption that individuals are detected independently of one another       
      2. Group structure with each indivdiual assigned to a stratum (a site, or NS EW)     
           i. Individual level data set
            site[i] ~ dcat(site.probs[])   # site.probs[] depend on lambda[site[i]] - the Poisson mean of the abundance N[site[i]]   
            
            for(s in 1:nsites){
            N[s] ~ dpois(lambda[s])  
            log(lambda[s]) <- beta0 + beta1*habitat[s]  
            site.probs[s] <- lambda[s]/sum(lambda[])
            }
            
       3. Using DA data augmentation, the Poisson abundance model implies a categorical model for population (site) membership of each individual. But site membership is confounded with inclusion (the psi parameter for being a real unseen plant) as teh intercept of that Poisson mean. Solution is a constraint:
           i. psi <- sum(lambda[])/(nind+nz)    
       4. To include cluster or group size as one cypte of individual covariate, with log-linear model (Marques and Buckland, 2003)
           i. log(sigma[i]) = alpha0 + alpha1 * Covariate[i]     
           ii. Group size unknown for unobserved groups, need model like truncated Poisson (or negative binomial) also truncated at 1: OR easier in BUGS to substract 1 from observed group sizes so possible vales are 0, 1, 2, .... 
           iii. Note, not possible to accommodate indiviudal covariates in the unmarked package    
       5. To get total groups and total population size, multiple DA varaible z[i]*(group.size+1)   
           zg[i] <- z[i]*(groupsize[i] + 1)            
           
           
???
          N <- sum(z[]) 
          D <- N/1560  ### 2160      ## 180 meters, half width is 6 meters
          Ntot <- D * 6459           # the square meter density by the total m^2 of Picnic
      
```{r}

library(rjags)
library(R2jags)
library(AHMbook)


## What does habitat look like?  
data <- simHDSg(type = "line")
length(data$habitat) ## is a value for each site
nrow(data$data)

# Simulated data:
#   site: integer
#   y: all 1s for detected 
#   u1: u2 are directions for point DS

detections_all %>%
  filter(detected == 1) %>%
  filter(row_number() %in% 141:150)


x <- detections_all$distance[detections_all$detected == 1]  # real data, only observed
nind <- length(x)                                           # number of observed clusters
B <- 3                                                      # half-strip width   
nsites <- 2
habitat <- as.factor(c("NS","EW"))                          # continuous variable for each site, like percent cover, I have categorical, should still work
groupsize <- detections_all$size[detections_all$detected == 1] - 1    # so values follow a Poisson distribution and include zero, 1, 2, ...

# Augument data by allowing for pseudogroups, including the distance data and individual covariate data
M <- 400                                                    # Size of augmented data set
nz <- M - nind
y <- c(rep(1,nind), rep(0,nz))                              # capture indicator
# zoned <- c(detections_all$zoned, rep(NA, nz))half-width
site <- c(as.factor(detections_all$observer[detections_all$detected == 1]),rep(NA,nz))   # Don't know which site to add unobserved to
d <- c(x,rep(NA,nz))
groupsize <- c(groupsize, rep(NA, nz)) 
zst <- y                                                    # starting values for data augmentation variable

## Bundle data
jags.data <- list(y = y,  B = B, nind = nind, nsites = nsites, d= d, habitat = habitat,
                  site = site, nz = nz)


modelPicnic <- 
  paste("
        model {
        
        # Priors  
        alpha0 ~ dunif(-10,10)              # intercept for size
        alpha1 ~ dunif(-10,10)              # slope for size
        # beta0 ~ dunif(-10,10)               # intercept for habitat
        # beta1 ~ dunif(-10,10)               # slope for habitat
        
        beta1Dir ~ dunif(-10,10)            # offset for transect direction
        mu_int ~ dnorm(0, 0.0001)           # mean hyperparamter for random intercept
        sigma_int ~ dunif(0,100)            # SD hyperparameter for random intercepts
        tau_int <- 1/(sigma_int * sigma_int)# precision
        
        ## Random effect of direction (strata)
        for(s in 1:nsites){
          beta0Dir[s] ~ dnorm(mu_int, tau_int)  # random intercepts
        }
        
        lambda.group ~ dunif(-10,10)
        
        # psi is a derived parameter Data Augmentation for stratified populations where there are differences among populations
        # psi <- sum(lambda[])/(nind+nz)
        psi ~ dunif(0,1)       ## because we have two strata but are the same popuation so same chance of being included in either/both
        
        # Individual level model: observations and process  
        for(i in 1:(nind+nz)){
          z[i] ~ dbern(psi)                   # Data augmentation
          d[i] ~ dunif(0,B)                   # uniform distribution of population 
          groupsize[i] ~ dpois(lambda.group) # Group size is Poisson because we substracted one  
          
          log(sigma[i]) <- alpha0 + alpha1*groupsize[i] 
          p[i] <- z[i]*exp(-d[i]*d[i]/(2*sigma[i]*sigma[i]))  # detection depends on distance; half-normal  
          y[i] ~ dbern(p[i])
          
          site[i] ~ dcat(site.probs[1:nsites])          # Population distributed among sites, need to average, or treat at repeat measures
          zg[i] <- z[i]*(groupsize[i]+1)                # number of individuals in a group
          
        }
        
        for(s in 1:nsites){
          # Model for population size of groups 
          N[s] ~ dpois(lambda[s])
          log(lambda[s]) <- beta0Dir[s] + beta1Dir*habitat[s]   # no continuous covaraite, same habitat; factor direction
          site.probs[s] <- lambda[s]/sum(lambda[])
        }
        
        # Derived 
        G <- sum(z[])                  # Total number of groups
        Gavg <- sum(z[])/nsites        # Average number of groups 
        Ntotal <- sum(zg[])            # Total population size 
        Ntotavg <- sum(zg[])/nsites    # Average population size
        
        }")

writeLines(modelPicnic, "Picnic_hn.jags")
                  
ni <- 6000
nb <- 2000
nt <- 2
nc <- 3

inits <- function(){list(alpha0=0, alpha1 = 0.5, beta0Dir=0, beta1Dir=0, z=zst)}
params <- c("alpha0","alpha1","beta0Dir","beta1Dir","psi","Ntotal","Ntotavg","G","Gavg","lambda.group")

out <- jags(jags.data, inits, params, "Picnic_hn.jags", n.thin = nt, n.chains = nc, n.burnin = nb, n.iter = ni)





```


Try one at a time  
```{r}

picnic_simple <- detections_all %>%
  filter(detected == 1)

x <- picnic_simple$distance[picnic_simple$observer == "NS"]
B <- 3                                                         # Strip half-width
nind <- length(x)

## Continuous data using DA  
nz <- 200                       # Augment
y <- c(rep(1, nind), rep(0, nz))
x <- c(x, rep(NA,nz))  
coveredarea <- picnic.samples %>%
  mutate(Dir = gsub("[^[:alpha:]]", "", Sample.Label)) %>%
  group_by(Dir) %>%
  summarise(Area = 6*sum(Effort))

jags.data <- list(nind=nind, nz=nz, x=x, y=y, B=B, A = coveredarea$Area[coveredarea$Dir == "NS"])

modelNSPicnic <- 
  paste("
        model {
          
          # Priors
          sigma ~ dunif(0,1000)   # Half-normal scale
          psi ~ dunif(0,1)        # DA parameter
          
          # Likelihood
          for(i in 1:(nind+nz)){
            # Process model
            z[i] ~ dbern(psi)     # DA variables
            x[i] ~ dunif(0,B)     # Distribution of distances
            
            # Observation model
            logp[i] <- -((x[i]*x[i])/(2*sigma*sigma))    # Half-normal detection fct.
            p[i] <- exp(logp[i])
            mu[i] <- z[i]*p[i]    ## observation depends on includsion (z[i]) and detection distance (x[i]). Could add Direction? 
            y[i] ~ dbern(mu[i])   ## Bernoulli measurement error process
          }
          
          # Derived quantities
          N <- sum(z[1:(nind+nz)])       ## Population size, would need to reference to direction z[1:(nind[Dir[i]+nz[Dir[i]])] ?? Like family in LH
          D <- N/A
        }
        
        ")

writeLines(modelNSPicnic, "Picnic_NS.jags")

zst <- y
inits <- function(){ list(psi = runif(1), z=zst, sigma=runif(1,1,2))}

## Parameters to save
params <- c("N", "D","sigma")
# bd <- "C:/Users/deprengm/Downloads/winbugs143_unrestricted/winbugs14_full_patched/WinBUGS14/WinBUGS14/" # May have to adapt for your computer  
ni <- 11000
nt <- 2
nb <- 1000
nc <- 3
## Call JAGS from R
out2 <- jags(jags.data, inits, 
             params, "Picnic_NS.jags", 
             n.thin = nt, n.chains = nc, n.burnin = nb, n.iter = ni, 
             working.directory = getwd())

print(out2)

```



Nolan et al. 2023 Bobwhite   
```{r}

picnicMRDS <- 
  paste("
    model{
    
    #-------------------------------------------------------------------------------
    # Distance - Peak Power linear relationship
    #-------------------------------------------------------------------------------
    
     ### Priors ###
       a0 ~ dnorm(76.45, 0.076)	  #intercept 
       a1 ~ dnorm(-0.147, 1000)T(,0)  #growth rate
       tau <- pow(sigma1, -2) 	  #inverse of variance
       sigma1 ~ dexp(1/5)         
    
    
     ### Likelihood ###
       
       # Model to estimate a0, a1 and SD for relationship between peak power and distance
       # Using previously collected field data (Wilhite et al., 2020) (Ntest = 50) of peak power (pp_test) and distances (dist_test).
       # Switched distance and pp so can predict power as function of distance - can use power data later
          
       	for (i in 1:Ntest){
     	 	pp_test[i] ~ dnorm(Epower_test[i], tau)   	
    		Epower_test[i] <- a0 + a1*dist_test[i]
    		detprob_test[i] <- exp(-dist_test[i]^2/(2*sigma2^2))
                    detected[i] ~ dbern(detprob_test[i])}
    
    
    
    #-------------------------------------------------------------------------------
    # ARU Call Detection and Abundance Model
    #-------------------------------------------------------------------------------
    
      ### Priors ###
    
        sigma2 ~ dnorm(200,0.01)
        CallRate ~ dnorm(42.97, 0.5)    
        fp ~ dnorm(0.165, 1000)T(0,)
    
      ### Likelihood ###
        
    	for(t in 1:nYear){
    
    			psi[t] ~ dunif(0,1) 
    			
        		for(i in 1:nSites[t]){
    		    
             	   for (j in 1:(M)){ 	 	
    
    
          			dist[j,i,t] ~ dunif(0, dmax)                              
          			negLogLike[j,i,t] = -log(2*dist[j,i,t] / dmax^2)
          			zeros[j,i,t] ~ dpois(negLogLike[j,i,t])
                           
    			Epower[j,i,t] <- a0 + a1*dist[j,i,t]
    			pp_real[j,i,t] ~ dnorm(Epower[j,i,t], tau) 
    	           	p_ind[j,i,t] <- exp(-(dist[j,i,t]*dist[j,i,t])/(2*sigma2^2))  
    			
          			z[j,i,t] ~ dbern(psi[t])                                   
          			mu[j,i,t] <- z[j,i,t]*p_ind[j,i,t]
          			y[j,i,t] ~ dbern(mu[j,i,t])                               
    
    	          }#j
    
       	     NsiteFP[i,t] <- sum(z[,i,t])                             # Realized number of calls at each site, including false positives
       	     Nsite[i,t] ~ dbin(1-fp, NsiteFP[i,t])                    # Realized number of calls at each site
        	     ENsite[i,t] <- M*psi[t]*(1-fp)
    
    
    		}#i
    		
    	N[t] <- sum(Nsite[1:nSites[t],t])                             # Realized number of calls in year t
      	D.r[t] <- ((N[t]/CallRate)*10) / area[t]                      # Realized quail density
            
    
    	    }#t
    
        Pdet2016 <- mean(p_ind[,1:33,1])                           # Mean probability of detection
        Pdet2017 <- mean(p_ind[,34:48,2])                          # Mean probability of detection 
        mean_Pdet <- mean(c(Pdet2016,Pdet2017))
          
    
    }
")

```












### Observational Error

```{r echo=FALSE}

ob_error_picnic <- (nrow(filter(picnic_duped, duplicated(plant_id)))*2 / nrow(filter(picnic_duped, zoned == 1)))

print(paste("Observational error =", ob_error_picnic, sep = " "))
print(paste("Actual number of unique cactus = ", n_distinct(picnic_duped$plant_id), sep = " " ))
#should be the corrected number of observations. 
print( paste("Corrected number of observations =",
             mean( c(n_distinct(which(picnic_duped$transect_orientation == "NS"))/
                       ob_error_picnic, 
                     n_distinct(which(picnic_duped$transect_orientation == "EW")) /
                       ob_error_picnic)) , sep = " ")) 

```

### Changing Fuzziness

```{r}

#make a curve that shows how the number of corrected observations changes as you change the fuzziness

fuzziness <- c(seq(0, 3, by = 0.1))
 
      error = fuzziness
      n_duplicated = fuzziness
      n_actual = fuzziness 
      n_corrected = fuzziness
    
    
for (fuzz in fuzziness){

picnic_crazy <- crossed(picnic_id, fuzz)

temp_error <-  which(error == as.character(fuzz))  
   error[which(error == as.character(fuzz))] <- (nrow(filter(picnic_crazy, duplicated(plant_id)))*2 / nrow(filter(picnic_id, zoned == 1)))

   n_duplicated[which(n_duplicated == as.character(fuzz))] <- sum(duplicated(picnic_crazy$plant_id))
    
   n_actual[which(n_actual == as.character(fuzz))] <- n_distinct(picnic_crazy$plant_id)
   
   n_corrected[temp_error] <- mean( c(n_distinct(which(picnic_crazy$transect_orientation == "NS"))/error[temp_error],
                                       n_distinct(which(picnic_crazy$transect_orientation == "EW"))/error[temp_error]))

}
      
picnic_fuzz <- tibble(fuzziness, error, n_actual, n_duplicated, n_corrected)
 
```

```{r}

pivot_longer(picnic_fuzz, cols = !fuzziness, names_to= "type", values_to= "count") %>% 
  group_by(type) %>% 
  ggplot( aes(fuzziness, count, color = type))+
  geom_line() +
  labs( x = "Fuzziness (m)")



```

## Site: OilPad

```{r}


oilpad_lines <- tibble( x = c(-20, 0, 0),
                        y = c(0, 20, 40),
                        xend = c(-20, -60, -60), 
                        yend = c(60, 20, 40))
oilpad_1 <- oilpad %>% 
  mutate( x = case_when(transect_orientation == "EW" ~ x, 
                        transect_orientation == "NS" ~ x - 20),
          y = case_when(transect_number == 1 ~ y + 20,
                        transect_number == 2 & transect_orientation == "EW" ~ y+40,
                        transect_number == 21 & transect_orientation == "EW" ~ y+40,
                        transect_orientation == "NS" ~ y)
          )

oilpad_plot <- oilpad_1 %>% 
  ggplot( aes(x,y)) +
  geom_point( aes(color = transect_number)) +
  geom_segment(data = oilpad_lines, aes(x = x, y = y, xend = xend, yend = yend))

oilpad_plot


```

Oil pad had double observers along the EW transects, so duplicates will need to be made along the same transect ( 2EW = 21EW & 2NS = 21NS)

```{r}

# `site` = a dataframe with relative (x,y) coords as seperate columns `x` & `y`, transect directions as `NS` & `EW`, and unique `plant_id' for each observation.
# `b` = an allowance in meters(m) within which two observations (1 NS & 1 EW) will be considered the same observation
oilpad_id <- mutate(oilpad_1, plant_id = c(1:nrow(oilpad_1)))

 temp_site <-  oilpad_id %>% 
    rowwise() %>% 
    mutate(coord = list(c(x,y)))
  
#chart all possible combinations of NS coordinates and EW coordinates
 grid <-  expand.grid(original = temp_site[which( oilpad_id$transect_number == 2), "coord"]$coord,
                      double = temp_site[which( oilpad_id$transect_number == 21), "coord"]$coord) %>% 
    rowwise() %>% 
   #make a column of distances between points
    mutate( dist = sqrt(sum((original - double)^2))) %>% 
   #filter the combinations to only the ones within the supplied fuzziness `b`
    filter(dist <= 1)

#appending to contain columns with EW plant_id's for each combination
grid <- left_join(grid, temp_site[, c("coord", "plant_id")],
                  by = join_by(original == coord)) %>% 
  mutate(original_ID = plant_id, plant_id = NULL) %>% 
#appending to contain columns with NS plant_id's for each combination
        left_join(temp_site[, c("coord", "plant_id")], 
                  by = join_by(double == coord)) %>% 
  mutate(double_ID = plant_id, plant_id = NULL)

#checking for Id's with multiple matches.(Should be only one distinct NS observation for each distinct EW observation). Keep only the match with the least distance between points
if(!all(count(grid, original_ID)$n >1)){
grid <- filter(group_by(grid, double_ID), dist == min(dist))
grid <- filter(group_by(grid, double_ID), dist == min(dist))
}

#Within the original dataframe, change corresponding EW ID's to their NS counterparts
temp_site <- left_join(temp_site, grid[, c("original_ID", "double_ID")], by = join_by(plant_id == original_ID)) 
temp_site <- temp_site %>% 
  mutate( plant_id = ifelse( !is.na(double_ID), double_ID, plant_id), double_ID = NULL, coord = NULL) %>% 
  ungroup()
  
temp_site -> oilpad_duped
rm(temp_site, grid)  
 



```

```{r}
oilpad_plot +
  geom_point(data =  filter(oilpad_duped, duplicated(plant_id)), aes(x,y), color = "blue") +
  geom_text_repel(data = oilpad_duped, aes( label = plant_id), size = 3, min.segment.length = 0.1)
```

Possible duplicates with mistyped X coordinates are {plant 6 (-3.57, 39.36) & plant 8 (-8.27, 39.00)} and {plant 9 (-0.56, 40.15) & plant 7(-9.57, 40.17)}.

## Site: Bridgeport

### Plotting individuals

```{r}
bridgeport_lines <- tibble( x = c(-30, -40, -50, 0, 0, 0, 0),
                            y = c( 0, 0, 0, 10, 20, 30, 40),
                            xend = c(-30, -40, -50, -53, -53, -53, -53),
                            yend = c(50, 50, 40, 10, 20, 30, 40 )
)

bridgeport_1 <- bridgeport %>% 
  mutate(x = case_when( transect_orientation == "NS" & transect_number == 1 ~ x-30,
                        transect_orientation == "NS" & transect_number == 2 ~ x-40,
                        transect_orientation == "NS" & transect_number == 3 ~ x-50,
                        TRUE ~ x
                        ),
         y = case_when( transect_orientation == "EW" & transect_number == 1 ~ y+10,
                        transect_orientation == "EW" & transect_number == 2 ~ y+20,
                        transect_orientation == "EW" & transect_number == 3 ~ y+30,
                        TRUE ~ y
                        )
         )



bridgeport_plot <- bridgeport_1 %>%
  ggplot(aes(x,y))+
  geom_point(aes(color = transect_orientation))+
  geom_segment(data = bridgeport_lines, aes(x = x, y = y, xend = xend, yend = yend))

bridgeport_plot
```

### Marking Overlap

```{r}
bridgeport_id <- mutate(bridgeport_1, plant_id = c(1:nrow(bridgeport_1)))

#filter to crozzing zones
bridgeport_id <- mutate(bridgeport_id, zoned = case_when( c(rowSums(mapply( FUN = function(a, b){between(a, b-3, b+3)}, bridgeport_id[, "y"], c(10, 20, 30, 40))) +
                                                    rowSums(mapply( FUN = function(a, b){between(a, b-3, b+3)}, bridgeport_id[, "x"], c(-30, -40, -50)))) == 2 ~ 1,
       TRUE ~ 0)) 

#find duplicates within those zones
bridgeport_duped <- crossed(filter(bridgeport_id), 0.5)

bridgeport_plot +
  geom_point(data =  filter(bridgeport_duped, duplicated(plant_id)), aes(x,y), color = "blue") +
  geom_text_repel(data = filter(bridgeport_duped, duplicated(plant_id)), aes( label = plant_id), size = 3, min.segment.length = 0.1)
```

### Observational Error

```{r}
ob_error_bridgeport <- (nrow(filter(bridgeport_duped, duplicated(plant_id)))*2 / 
                        nrow(filter(bridgeport_duped, zoned == 1)))

print(paste("Observational error =", ob_error_bridgeport, sep = " "))
print(paste("Actual number of unique cactus = ", n_distinct(bridgeport_duped$plant_id), sep = " " ))
#should be the corrected number of observations. 
print( paste("Corrected number of observations =",
             mean( c(n_distinct(which(bridgeport_duped$transect_orientation == "NS"))/
                       ob_error_bridgeport, 
                     n_distinct(which(bridgeport_duped$transect_orientation == "EW")) /
                       ob_error_bridgeport)) , sep = " ")) 
```

## Notes and Errors

-   There is an issue with this `change in fuzziness` section when using larger `fuzziness` in which the `n_duplicated` entries start to seemingly be rewritten to larger amounts than should actually be calculated. This issue is not present in the `crossed` function. The issue only occurs when looping the `crossed` function and writing the `*_fuzz` tables, so probably an issue with how `n_duplicated` is being written or called.

-   Ideally the allowance for overlapping observations should be within 0.1m or 10cm as that is the distance in which we counted multiple cactus to be a part of a single group/observation. Using this allowance generates an abnormally small set of overlapping observations. Causes are possibly imprecise data collection, differences in actual plot set-up and recorded information, wind blowing transect lines resulting in differing measurements

-   Further work correlating the covariates (group size, reproductive status, distance from observer) with observational biases should be done.

------------------------------------------------------------------------

Density of observations from 0 to 3m away from line.

```{r}
 
scgl1 %>% 
  filter(abs(distance_to_group) <= 4) %>% 
  ggplot(aes( x = abs(distance_to_group), fill = transect_orientation)) +
  geom_histogram(bins = 30, alpha = 0.3 )+
  facet_wrap( facets = vars(site_name))
# note: site 3 = Picnic, site 12 = Fram


```

Picnic (site 3) has a disproportionate amount of EW observations, but the trend matches the assumption that less observations will be made as the distance from the observer increases.

The Distance:Count trend is less obvious at Fram (site 12).

------------------------------------------------------------------------

Looking at correlation between any amount of flowering and overlapping observations

```{r}

# dupe = is it an overlapping observation?
# flower = is it flowering at all?
tibble( duped_ratio = (fram_duped %>% 
  mutate(dupe = ifelse(plant_id %in% fram_duped[which(duplicated(plant_id)),]$plant_id, 1, 0), 
         flower = ifelse(number_reproductive_in_group >= 1, 1, 0)) %>%
  filter(dupe == 1) %>% 
  mutate( flower = ifelse( is.na(flower), 0, flower)) %>% 
  summarize( "Duped_ratio" = sum(flower)/sum(dupe)))$Duped_ratio,

 total_ratio = (fram_duped %>% 
  mutate(dupe = ifelse(plant_id %in% fram_duped[which(duplicated(plant_id)),]$plant_id, 1, 0), 
         flower = ifelse(number_reproductive_in_group >= 1, 1, 0)) %>%
  mutate( flower = ifelse( is.na(flower), 0, flower)) %>% 
  summarize( "Total_ratio" = sum(flower)/n_distinct(fram_duped$plant_id)))$Total_ratio
)
# shows that, at Fram, 75% of the overlapping observations had at least 1 reproductive individual in the group
```

```{r}
tibble( duped_ratio = (picnic_duped %>% 
  mutate(dupe = ifelse(plant_id %in% picnic_duped[which(duplicated(plant_id)),]$plant_id, 1, 0), 
         flower = ifelse(number_reproductive_in_group >= 1, 1, 0)) %>%
  filter(dupe == 1) %>% 
  mutate( flower = ifelse( is.na(flower), 0, flower)) %>% 
  summarize( "duped_ratio" = sum(flower)/sum(dupe)))$duped_ratio,
  
total_ratio = (picnic_duped %>% 
  mutate(dupe = ifelse(plant_id %in% picnic_duped[which(duplicated(plant_id)),]$plant_id, 1, 0), 
         flower = ifelse(number_reproductive_in_group >= 1, 1, 0)) %>%
  mutate( flower = ifelse( is.na(flower), 0, flower)) %>% 
  summarize( "Total_ratio" = sum(flower)/n_distinct(picnic_duped$plant_id)))$Total_ratio
)
```
